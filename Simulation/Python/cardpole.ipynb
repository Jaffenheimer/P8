{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Custom Environment\n",
    "import gym\n",
    "import numpy as np\n",
    "import traci\n",
    "import math\n",
    "from os import path\n",
    "\n",
    "class SumoEnv(gym.Env):\n",
    "  def __init__(self):\n",
    "    traci.start([\"sumo\", \"-c\", path.abspath(\"../SUMO/test.sumocfg\")])\n",
    "    \n",
    "    ## SUMO VARIABLES ##\n",
    "    self.bus_stop_positions = [[123, 974, 1872, 2764], [123, 827, 1742, 2702, 3592]]\n",
    "    self.bus_locations={\"-overlap\": \"123\", \"-R2\": \"259\", \"-R1\": \"125\", \"-R0\": \"267\", \"-L3\": \"117\", \"-L2\": \"110\", \"-L1\": \"123\", \"-L0\": \"120\"}\n",
    "    self.bus_ids=[\"bus_r_0_0\", \"bus_r_0_1\", \"bus_r_0_2\", \"bus_r_0_3\", \"bus_r_0_4\"]\n",
    "    \n",
    "    # self.route_names = [[\"-overlap\", \"-R2\", \"-R1\", \"-R0\"], [\"-overlap\", \"-L3\", \"-L2\", \"-L1\", \"-L0\"]]\n",
    "    self.route_lengths = [3591, 4697]\n",
    "    # self.route_junctions={\"J1\": [\"-L0\", \"-R0\", \"-overlap\"], \"J2\": [\"-R0\", \"-R1\"], \"J3\": [\"-R1\", \"-R2\"], \"J4\": [\"-L3\", \"-R2\", \"-overlap\"], \"J5\": [\"-L0\", \"-L1\"], \"J7\": [\"-L1\", \"-L2\"], \"J8\": [\"-L2\", \"-L3\"]}\n",
    "    \n",
    "    self.wait_time = 0\n",
    "    self.delta_speed = 0.1\n",
    "    self.min_speed_before_change = 30\n",
    "    self.action_delta_speed = {0: (1-self.delta_speed), 2: (1+self.delta_speed)}\n",
    "\n",
    "    ## GYM VARIABLES ##\n",
    "    self.bus_num = 5\n",
    "    bus_stops_num = 4\n",
    "    bus_speed_max = 50\n",
    "\n",
    "    #actions: [b1, b2 (...)] # each action is either 0 = slow down, 1 = keep speed, 2 = speed up\n",
    "    self.action_space = gym.spaces.Box(low=np.array([0]*self.bus_num), high=np.array([2]*self.bus_num), shape=(self.bus_num,), dtype=np.float32)\n",
    "    \n",
    "    #states: [avg_wait_time, b1_speed, b1_pos, b2_speed, b2_pos, (...),  bs1_pos, bs2_pos, bs3_pos, bs4_pos]\n",
    "    wait_max = 100000\n",
    "    low_obs = np.zeros([1 + 2*self.bus_num + bus_stops_num])\n",
    "    high_obs = np.array([wait_max] + [bus_speed_max, self.route_lengths[0]]*self.bus_num + [self.route_lengths[0]]*bus_stops_num)\n",
    "    self.observation_space = gym.spaces.Box(low=low_obs, high=high_obs, shape=(1 + 2*self.bus_num + bus_stops_num,), dtype=np.float32)\n",
    "\n",
    "    self.max_steps = 500\n",
    "    self.current_step = 0\n",
    "\n",
    "  def reset(self):\n",
    "    pass\n",
    "  \n",
    "  def step(self, action):\n",
    "    try:\n",
    "      next_state = self.sumo_step()\n",
    "\n",
    "      #set action for each bus: 0 = slow down, 1 = keep speed, 2 = speed up\n",
    "      vehicles_length = len(traci.vehicle.getIDList())\n",
    "      \n",
    "      for i, bus_action in enumerate(action):\n",
    "        if bus_action == 1 or i >= vehicles_length: break\n",
    "        bus_id = self.bus_ids[i]\n",
    "        bus_distance_driven = traci.vehicle.getDistance(bus_id)\n",
    "        \n",
    "        if np.sign(bus_distance_driven) == -1: break ## if bus hasnt driven yet, skip\n",
    "\n",
    "        bus_route = traci.vehicle.getRouteID(bus_id)\n",
    "        bus_position = round(bus_distance_driven % (self.route_lengths[0] if (bus_route == \"r_0\") else self.route_lengths[1]), 3)\n",
    "        nearest_bus_stop_position = self._find_nearest(self.bus_stop_positions[0 if bus_route == \"r_0\" else 1], bus_position)\n",
    "        bus_speed_km_t = traci.vehicle.getSpeed(bus_id)*3.6 # m/s to km/h\n",
    "        \n",
    "        interval = [-22, 3]\n",
    "        # change speed if speed > min_speed_before_change and bus is not at a bus stop\n",
    "        if bus_speed_km_t > self.min_speed_before_change and not (bus_position > nearest_bus_stop_position + interval[0] and bus_position < nearest_bus_stop_position + interval[1]):\n",
    "          new_speed = self.action_delta_speed[bus_action] * traci.vehicle.getSpeed(bus_id) # speed is in m/s\n",
    "          traci.vehicle.slowDown(bus_id, new_speed, 1) # smoothly changes to new speed over 1 second\n",
    "\n",
    "      # reward are given if the new waiting time is strictly lower, otherwise punished\n",
    "      reward = 1 if next_state[0] < self.wait_time else -1\n",
    "\n",
    "      # set the wait time to the current wait time\n",
    "      self.wait_time = next_state[0]\n",
    "\n",
    "      # check if done\n",
    "      self.current_step += 1\n",
    "      done = False\n",
    "      if(self.current_step >= self.max_steps):\n",
    "        done = True\n",
    "\n",
    "      return next_state, reward, done, {}\n",
    "\n",
    "    except Exception as e: # if there is an error, close the simulation\n",
    "      print(\"An error occurred. Closing simulation.\")\n",
    "      print(\"Error: \", e)\n",
    "      traci.close()\n",
    "    \n",
    "  def render(self):\n",
    "    pass\n",
    "\n",
    "  def close(self):\n",
    "    traci.close()\n",
    "\n",
    "  def seed(self, seed=None):\n",
    "    pass\n",
    "\n",
    "  ## SUMO FUNCTIONS\n",
    "  def sumo_step(self):\n",
    "    # state [avg_wait_time, b1_speed, b1_pos, b2_speed, b2_pos, (...),  bs1_pos, bs2_pos, bs3_pos, bs4_pos]\n",
    "    new_state = [0] * (1 + 2 * self.bus_num) + self.bus_stop_positions[0]\n",
    "    personsWaitingTimeList = []\n",
    "    traci.simulationStep()\n",
    "\n",
    "    vehicles = traci.vehicle.getIDList()\n",
    "    persons = traci.person.getIDList()\n",
    "\n",
    "    ## finds the average waiting time\n",
    "    for i in range(0,len(persons)):\n",
    "      personWaitingTime = traci.person.getWaitingTime(persons[i])\n",
    "      personsWaitingTimeList.append(personWaitingTime)\n",
    "\n",
    "    persons_waiting_num = len(personsWaitingTimeList)\n",
    "    new_state[0] = round(sum(personsWaitingTimeList) / persons_waiting_num, 3) if persons_waiting_num > 0 or not np.isnan(persons_waiting_num) or not np.isnan(personsWaitingTimeList) else 0.0\n",
    "\n",
    "    ## finds bus speed and position\n",
    "    for j in range(0,len(vehicles)):\n",
    "      vehicleId = vehicles[j]\n",
    "      if traci.vehicle.getRouteID(vehicleId) != \"r_0\": continue\n",
    "\n",
    "      vehicleSpeed = traci.vehicle.getSpeed(vehicleId)*3.6 # m/s to km/h\n",
    "      vehiclePosition = traci.vehicle.getDistance(vehicleId) % (self.route_lengths[0] \n",
    "                        if (traci.vehicle.getRouteID(vehicleId) == \"r_0\") else self.route_lengths[1])\n",
    "      new_state[1 + 2*j] = round(vehicleSpeed, 2)\n",
    "      new_state[2 + 2*j] = round(vehiclePosition, 2)\n",
    "    return new_state\n",
    "  \n",
    "  def _find_nearest(self, array, value):\n",
    "    idx = np.searchsorted(array, value, side=\"left\")\n",
    "    if (idx == len(array) and math.fabs(value - (array[0] + array[idx-1])) < math.fabs(value - array[idx-1])):\n",
    "      print(\"first case!\")\n",
    "      return array[0]\n",
    "    elif idx > 0 and idx == len(array) or math.fabs(value - array[idx-1]) < math.fabs(value - array[idx]):\n",
    "      print(\"second case!\")\n",
    "      return array[idx-1]\n",
    "    else:\n",
    "      print(\"last case!\")\n",
    "      return array[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "traci.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sebas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gym\\spaces\\box.py:127: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    }
   ],
   "source": [
    "env = SumoEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sebas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gym\\spaces\\box.py:127: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first case!\n",
      "second case!\n",
      "second case!\n",
      "second case!\n",
      "last case!\n",
      "second case!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 7.253s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x2479cb18e90>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "class TestSumoEnv(unittest.TestCase):\n",
    "  def setUp(self): # setup the variables here for every test\n",
    "    self.env = SumoEnv()\n",
    "\n",
    "  def test_initializations(self):\n",
    "    self.env.reset()\n",
    "    action = [1, 1, 1, 1, 1]\n",
    "    state, reward, done, _ = self.env.step(action)\n",
    "    self.assertEqual(len(state), 15)\n",
    "    self.assertEqual(state, [0.0, 0.0, 0.0, 0, 0, 0, 0, 0, 0, 0, 0, 123, 974, 1872, 2764])\n",
    "    self.assertEqual(reward, -1)\n",
    "    self.assertFalse(done)\n",
    "    self.env.close()\n",
    "\n",
    "  def test_find_nearest_function(self):\n",
    "    self.env.reset()\n",
    "    self.assertEqual(self.env._find_nearest([350, 700, 950, 1600, 2600], 3100), 350) # goes into first if statement\n",
    "    self.assertEqual(self.env._find_nearest([350, 700, 950, 1600, 2600], 2000), 1600) # goes into second if statement\n",
    "    self.assertEqual(self.env._find_nearest([350, 700, 950, 1600, 2600], 2700), 2600) # goes into second if statement\n",
    "    self.assertEqual(self.env._find_nearest([2, 4, 6, 8, 10], 11), 10) # goes into second if statement\n",
    "    self.assertEqual(self.env._find_nearest([2, 4, 6, 8, 10], 3.6), 4) # goes into else statement\n",
    "    self.env.close()\n",
    "\n",
    "unittest.main(argv=[''], exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sebas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gym\\spaces\\box.py:127: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    }
   ],
   "source": [
    "traci.close()\n",
    "env = SumoEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected parameter loc (Tensor of shape (2, 5)) of distribution Normal(loc: torch.Size([2, 5]), scale: torch.Size([2, 5])) to satisfy the constraint Real(), but found invalid values:\ntensor([[nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan]])",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m vec_env \u001b[38;5;241m=\u001b[39m make_vec_env(\u001b[38;5;28;01mlambda\u001b[39;00m: env, n_envs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m PPO(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMlpPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m, vec_env, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m, policy_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mdict\u001b[39m(net_arch\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m64\u001b[39m]))\n\u001b[1;32m----> 6\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mppo_sumo\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m model\n",
      "File \u001b[1;32mc:\\Users\\sebas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:315\u001b[0m, in \u001b[0;36mPPO.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    307\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[0;32m    308\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    313\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    314\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[1;32m--> 315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sebas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:300\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    299\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[1;32m--> 300\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    302\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[0;32m    303\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sebas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:179\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[1;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m th\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m    177\u001b[0m     \u001b[38;5;66;03m# Convert to pytorch tensor or to TensorDict\u001b[39;00m\n\u001b[0;32m    178\u001b[0m     obs_tensor \u001b[38;5;241m=\u001b[39m obs_as_tensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m--> 179\u001b[0m     actions, values, log_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m actions \u001b[38;5;241m=\u001b[39m actions\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m    182\u001b[0m \u001b[38;5;66;03m# Rescale and perform action\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sebas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sebas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sebas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stable_baselines3\\common\\policies.py:654\u001b[0m, in \u001b[0;36mActorCriticPolicy.forward\u001b[1;34m(self, obs, deterministic)\u001b[0m\n\u001b[0;32m    652\u001b[0m \u001b[38;5;66;03m# Evaluate the values for the given observations\u001b[39;00m\n\u001b[0;32m    653\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_net(latent_vf)\n\u001b[1;32m--> 654\u001b[0m distribution \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_action_dist_from_latent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatent_pi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    655\u001b[0m actions \u001b[38;5;241m=\u001b[39m distribution\u001b[38;5;241m.\u001b[39mget_actions(deterministic\u001b[38;5;241m=\u001b[39mdeterministic)\n\u001b[0;32m    656\u001b[0m log_prob \u001b[38;5;241m=\u001b[39m distribution\u001b[38;5;241m.\u001b[39mlog_prob(actions)\n",
      "File \u001b[1;32mc:\\Users\\sebas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stable_baselines3\\common\\policies.py:694\u001b[0m, in \u001b[0;36mActorCriticPolicy._get_action_dist_from_latent\u001b[1;34m(self, latent_pi)\u001b[0m\n\u001b[0;32m    691\u001b[0m mean_actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_net(latent_pi)\n\u001b[0;32m    693\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_dist, DiagGaussianDistribution):\n\u001b[1;32m--> 694\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_dist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproba_distribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmean_actions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_std\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    695\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_dist, CategoricalDistribution):\n\u001b[0;32m    696\u001b[0m     \u001b[38;5;66;03m# Here mean_actions are the logits before the softmax\u001b[39;00m\n\u001b[0;32m    697\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_dist\u001b[38;5;241m.\u001b[39mproba_distribution(action_logits\u001b[38;5;241m=\u001b[39mmean_actions)\n",
      "File \u001b[1;32mc:\\Users\\sebas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stable_baselines3\\common\\distributions.py:164\u001b[0m, in \u001b[0;36mDiagGaussianDistribution.proba_distribution\u001b[1;34m(self, mean_actions, log_std)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;124;03mCreate the distribution given its parameters (mean, std)\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;124;03m:return:\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    163\u001b[0m action_std \u001b[38;5;241m=\u001b[39m th\u001b[38;5;241m.\u001b[39mones_like(mean_actions) \u001b[38;5;241m*\u001b[39m log_std\u001b[38;5;241m.\u001b[39mexp()\n\u001b[1;32m--> 164\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribution \u001b[38;5;241m=\u001b[39m \u001b[43mNormal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmean_actions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction_std\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\sebas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\distributions\\normal.py:56\u001b[0m, in \u001b[0;36mNormal.__init__\u001b[1;34m(self, loc, scale, validate_args)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     55\u001b[0m     batch_shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloc\u001b[38;5;241m.\u001b[39msize()\n\u001b[1;32m---> 56\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sebas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\distributions\\distribution.py:68\u001b[0m, in \u001b[0;36mDistribution.__init__\u001b[1;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[0;32m     66\u001b[0m         valid \u001b[38;5;241m=\u001b[39m constraint\u001b[38;5;241m.\u001b[39mcheck(value)\n\u001b[0;32m     67\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m valid\u001b[38;5;241m.\u001b[39mall():\n\u001b[1;32m---> 68\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     69\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected parameter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     70\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(value\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     71\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof distribution \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     72\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto satisfy the constraint \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(constraint)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     73\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut found invalid values:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     74\u001b[0m             )\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[1;31mValueError\u001b[0m: Expected parameter loc (Tensor of shape (2, 5)) of distribution Normal(loc: torch.Size([2, 5]), scale: torch.Size([2, 5])) to satisfy the constraint Real(), but found invalid values:\ntensor([[nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan]])"
     ]
    }
   ],
   "source": [
    "# from stable_baselines3 import PPO\n",
    "# from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "# vec_env = make_vec_env(lambda: env, n_envs=2)\n",
    "# model = PPO(\"MlpPolicy\", vec_env, verbose=1, learning_rate=0.001, policy_kwargs=dict(net_arch=[64, 64]))\n",
    "# model.learn(total_timesteps=10000)\n",
    "# model.save(\"ppo_sumo\")\n",
    "\n",
    "# del model\n",
    "\n",
    "# model = PPO.load(\"ppo_sumo\")\n",
    "\n",
    "# obs = vec_env.reset()\n",
    "# while True:\n",
    "#   action, _states = model.predict(obs)\n",
    "#   obs, rewards, dones, info = vec_env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109:  [37.561, 0.0, 978.28, 50.0, 448.69, 26.94, 34.54, 0, 0, 0, 0, 123, 974, 1872, 2764] 1 False {}\n"
     ]
    }
   ],
   "source": [
    "while env.current_step < 50:\n",
    "  next_state, reward, done, info = env.step([1]*env.bus_num)\n",
    "\n",
    "next_state, reward, done, info = env.step([2]*env.bus_num)\n",
    "print(f\"{env.current_step}: \", next_state, reward, done, info)\n",
    "\n",
    "# state [avg_wait_time, b1_speed, b1_pos, b2_speed, b2_pos, (...),  bs1_pos, bs2_pos, bs3_pos, bs4_pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traci.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Custom Environment\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "class ShopEnv(gym.Env):\n",
    "  def __init__(self):\n",
    "\n",
    "    self.max_capacity = 4000\n",
    "    self.action_space = gym.spaces.Box(low=np.array([0]), high=np.array([self.max_capacity]), shape=(1,), dtype=np.float32)\n",
    "    self.lead_time = 5 # controls the shape of the observation space\n",
    "    self.obs_dim = 10 + self.lead_time\n",
    "    obs_low = np.zeros([self.obs_dim])\n",
    "\n",
    "    self.max_mean_daily_demand = 1000\n",
    "    self.max_daily_holding_cost_per_unit = 20\n",
    "    self.max_unit_selling_price = 5\n",
    "\n",
    "    obs_high = np.array([self.max_capacity for _ in range(self.lead_time)] + [self.max_mean_daily_demand, self.max_unit_selling_price, self.max_daily_holding_cost_per_unit])\n",
    "    self.observation_space = gym.spaces.Box(low=obs_low, high=obs_high, dtype=np.float32)\n",
    "\n",
    "    self.rng = np.random.default_rng()\n",
    "\n",
    "    self.current_obs = None\n",
    "    self.episode_length_in_days = 90\n",
    "    self.day_num = None\n",
    "\n",
    "  def reset(self):\n",
    "    mean_daily_demand = self.rng.uniform() * self.max_mean_daily_demand\n",
    "\n",
    "    selling_price =  self.rng.uniform() * self.max_unit_selling_price\n",
    "    buying_price = self.rng.uniform() * selling_price\n",
    "\n",
    "    daily_holding_cost_per_unit = self.rng.uniform() * min(buying_price, self.max_daily_holding_cost_per_unit)\n",
    "\n",
    "    self.current_obs = np.array([0 for _ in range(self.lead_time)] + [mean_daily_demand, selling_price, daily_holding_cost_per_unit])\n",
    "    \n",
    "    self.day_num = 0\n",
    "    return self.current_obs\n",
    "  \n",
    "  def step(self, action):\n",
    "    buys = min(action[0], self.max_capacity - np.sum(self.current_obs[:self.lead_time]))\n",
    "    demand = self.rng.poisson(self.current_obs[self.lead_time])\n",
    "    next_obs = np.concatenate((self.current_obs[1: self.lead_time], np.array([buys]), self.current_obs[self.lead_time:]))\n",
    "    next_obs[0] += max(0, self.current_obs[0] - demand) # a part of an equation\n",
    "\n",
    "    reward = (self.current_obs[self.lead_time + 1] * (self.current_obs[0] + self.current_obs[1] - next_obs[0]) - self.current_obs[self.lead_time + 2] * buys - self.current_obs[self.lead_time + 3] * (next_obs[0] - self.current_obs[1]))\n",
    "\n",
    "    self.day_num += 1\n",
    "    done = False\n",
    "    if self.day_num >= self.episode_length_in_days:\n",
    "      done = True\n",
    "    \n",
    "    self.current_obs = next_obs\n",
    "    \n",
    "    return self.current_obs, reward, done, # {} any additional information can be added to the agent here\n",
    "\n",
    "  def render(self, mode=\"human\"):\n",
    "    pass # the box representation is good enough, no need to render any image\n",
    "\n",
    "  def close(self):\n",
    "    pass\n",
    "\n",
    "  def seed(self, seed=None):\n",
    "    pass\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class ShowerEnv(Env):\n",
    "  def __init__(self):\n",
    "    self.action_space = Discrete(3)\n",
    "    self.observation_space = Box(low=np.array([0]), high=np.array([100]), dtype=np.float32)\n",
    "\n",
    "    self.state = 38 + random.randint(-3, 3)\n",
    "    self.shower_length = 60\n",
    "\n",
    "  def step(self, action):\n",
    "    self.state += action -1\n",
    "    self.shower_length -= 1\n",
    "\n",
    "    if self.state >= 37 and self.state <= 39:\n",
    "      reward = 1\n",
    "    else:\n",
    "      reward = -1\n",
    "    \n",
    "    if self.shower_length <= 0:\n",
    "      done = True\n",
    "    else:\n",
    "      done = False\n",
    "    \n",
    "    self.state += random.randint(-1, 1)\n",
    "    info = {}\n",
    "    return self.state, reward, done, info\n",
    "\n",
    "  def render(self):\n",
    "    pass\n",
    "  \n",
    "  def reset(self):\n",
    "    self.state = 38 + random.randint(-3, 3)\n",
    "    self.shower_length = 60\n",
    "    return [self.state], {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if GPU is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_observations, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, n_actions)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH_SIZE is the number of transitions sampled from the replay buffer\n",
    "# GAMMA is the discount factor as mentioned in the previous section\n",
    "# EPS_START is the starting value of epsilon\n",
    "# EPS_END is the final value of epsilon\n",
    "# EPS_DECAY controls the rate of exponential decay of epsilon, higher means a slower decay\n",
    "# TAU is the update rate of the target network\n",
    "# LR is the learning rate of the ``AdamW`` optimizer\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 1000\n",
    "TAU = 0.005\n",
    "LR = 1e-4\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "n_actions = env.action_space.n\n",
    "# Get the number of state observations\n",
    "state, info = env.reset()\n",
    "n_observations = len(state)\n",
    "\n",
    "policy_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # t.max(1) will return the largest column value of each row.\n",
    "            # second column on max result is index of where max element was\n",
    "            # found, so we pick action with the larger expected reward.\n",
    "            return policy_net(state).max(1).indices.view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)\n",
    "\n",
    "\n",
    "episode_durations = []\n",
    "\n",
    "\n",
    "def plot_durations(show_result=False):\n",
    "    plt.figure(1)\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    if show_result:\n",
    "        plt.title('Result')\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        if not show_result:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1).values\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1).values\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # In-place gradient clipping\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    num_episodes = 600\n",
    "else:\n",
    "    num_episodes = 10\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and get its state\n",
    "    state, info = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    for t in count():\n",
    "        action = select_action(state)\n",
    "        observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the policy network)\n",
    "        optimize_model()\n",
    "\n",
    "        # Soft update of the target network's weights\n",
    "        # θ′ ← τ θ + (1 −τ )θ′\n",
    "        target_net_state_dict = target_net.state_dict()\n",
    "        policy_net_state_dict = policy_net.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "        target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "        if done:\n",
    "            episode_durations.append(t + 1)\n",
    "            plot_durations()\n",
    "            break\n",
    "\n",
    "print('Complete')\n",
    "plot_durations(show_result=True)\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
