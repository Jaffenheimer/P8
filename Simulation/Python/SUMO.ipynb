{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for Sumo Enviroment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Environment\n",
    "import gym\n",
    "import numpy as np\n",
    "import traci\n",
    "import math\n",
    "from os import path\n",
    "\n",
    "\n",
    "class SumoEnv(gym.Env):\n",
    "  def __init__(self):\n",
    "    \n",
    "    traci.start([\"sumo\", \"-c\", path.abspath(\"../SUMO/test.sumocfg\")])\n",
    "\n",
    "    ## SUMO VARIABLES ##\n",
    "    self.bus_stop_positions = [\n",
    "        [123, 974, 1872, 2764], [123, 827, 1742, 2702, 3592]]\n",
    "    self.bus_locations = {\"-overlap\": \"123\", \"-R2\": \"259\", \"-R1\": \"125\",\n",
    "                          \"-R0\": \"267\", \"-L3\": \"117\", \"-L2\": \"110\", \"-L1\": \"123\", \"-L0\": \"120\"}\n",
    "    self.bus_ids = [\"bus_r_0_0\", \"bus_r_0_1\",\n",
    "                    \"bus_r_0_2\", \"bus_r_0_3\", \"bus_r_0_4\"]\n",
    "\n",
    "    # self.route_names = [[\"-overlap\", \"-R2\", \"-R1\", \"-R0\"], [\"-overlap\", \"-L3\", \"-L2\", \"-L1\", \"-L0\"]]\n",
    "    self.route_lengths = [3591, 4697]\n",
    "    # self.route_junctions={\"J1\": [\"-L0\", \"-R0\", \"-overlap\"], \"J2\": [\"-R0\", \"-R1\"], \"J3\": [\"-R1\", \"-R2\"], \"J4\": [\"-L3\", \"-R2\", \"-overlap\"], \"J5\": [\"-L0\", \"-L1\"], \"J7\": [\"-L1\", \"-L2\"], \"J8\": [\"-L2\", \"-L3\"]}\n",
    "\n",
    "    self.wait_time = 0\n",
    "    self.delta_speed = 0.1\n",
    "    self.min_speed_before_change = 30\n",
    "    self.action_delta_speed = {\n",
    "        0: (1-self.delta_speed), 2: (1+self.delta_speed)}\n",
    "\n",
    "    ## GYM VARIABLES ##\n",
    "    self.bus_num = 5\n",
    "    bus_stops_num = 4\n",
    "    bus_speed_max = 50\n",
    "\n",
    "    # actions: [b1, b2 (...)] # each action is either 0 = slow down, 1 = keep speed, 2 = speed up\n",
    "    self.action_space = gym.spaces.Box(low=np.array(\n",
    "        [0]*self.bus_num), high=np.array([2]*self.bus_num), shape=(self.bus_num,), dtype=np.int32)\n",
    "\n",
    "    # states: [avg_wait_time, b1_speed, b1_pos, b2_speed, b2_pos, (...),  bs1_pos, bs2_pos, bs3_pos, bs4_pos]\n",
    "    wait_max = 100000\n",
    "    low_obs = np.zeros([1 + 2*self.bus_num + bus_stops_num])\n",
    "    high_obs = np.array([wait_max] + [bus_speed_max, self.route_lengths[0]]\n",
    "                        * self.bus_num + [self.route_lengths[0]]*bus_stops_num)\n",
    "    self.observation_space = gym.spaces.Box(low=low_obs, high=high_obs, shape=(\n",
    "        1 + 2*self.bus_num + bus_stops_num,), dtype=np.float32)\n",
    "\n",
    "    self.max_steps = 500\n",
    "    self.current_step = 0\n",
    "\n",
    "  def reset(self):\n",
    "      traci.close()\n",
    "      self.wait_time = 0\n",
    "      self.current_step = 0\n",
    "      traci.start([\"sumo\", \"-c\", path.abspath(\"../SUMO/test.sumocfg\")])\n",
    "      return self.wait_time, {}\n",
    "\n",
    "  def step(self, action):\n",
    "    print(\"Action from env: \", action)\n",
    "    try:\n",
    "      next_state = self.sumo_step()\n",
    "\n",
    "      # set action for each bus: 0 = slow down, 1 = keep speed, 2 = speed up\n",
    "      vehicles_length = len(traci.vehicle.getIDList())\n",
    "    \n",
    "      for i, bus_action in enumerate(action):\n",
    "        if bus_action == 1 or i >= vehicles_length:\n",
    "          break\n",
    "        bus_id = self.bus_ids[i]\n",
    "        bus_distance_driven = traci.vehicle.getDistance(bus_id)\n",
    "\n",
    "        if np.sign(bus_distance_driven) == -1:\n",
    "          break  # if bus hasnt driven yet, skip\n",
    "\n",
    "        bus_route = traci.vehicle.getRouteID(bus_id)\n",
    "        bus_position = round(bus_distance_driven % (\n",
    "            self.route_lengths[0] if (bus_route == \"r_0\") else self.route_lengths[1]), 3)\n",
    "        nearest_bus_stop_position = self._find_nearest(\n",
    "            self.bus_stop_positions[0 if bus_route == \"r_0\" else 1], bus_position)\n",
    "        bus_speed_km_t = traci.vehicle.getSpeed(bus_id)*3.6  # m/s to km/h\n",
    "\n",
    "        interval = [-22, 3]\n",
    "        # change speed if speed > min_speed_before_change and bus is not at a bus stop\n",
    "        if bus_speed_km_t > self.min_speed_before_change and not (bus_position > nearest_bus_stop_position + interval[0] and bus_position < nearest_bus_stop_position + interval[1]):\n",
    "          # speed is in m/s\n",
    "          new_speed = self.action_delta_speed[bus_action] * \\\n",
    "              traci.vehicle.getSpeed(bus_id)\n",
    "          # smoothly changes to new speed over 1 second\n",
    "          traci.vehicle.slowDown(bus_id, new_speed, 1)\n",
    "\n",
    "      # reward are given if the new waiting time is strictly lower, otherwise punished\n",
    "      reward = 1 if next_state[0] < self.wait_time else -1\n",
    "\n",
    "      # set the wait time to the current wait time\n",
    "      self.wait_time = next_state[0]\n",
    "\n",
    "      # check if done\n",
    "      self.current_step += 1\n",
    "      done = False\n",
    "      if (self.current_step >= self.max_steps):\n",
    "        done = True\n",
    "\n",
    "      return next_state, reward, done, {}\n",
    "\n",
    "    except Exception as e:  # if there is an error, close the simulation\n",
    "      print(\"An error occurred. Closing simulation.\")\n",
    "      print(\"Error: \", e)\n",
    "      traci.close()\n",
    "\n",
    "  def render(self):\n",
    "    pass\n",
    "\n",
    "  def close(self):\n",
    "    pass\n",
    "\n",
    "  # SUMO FUNCTIONS\n",
    "  def sumo_step(self):\n",
    "    # state [avg_wait_time, b1_speed, b1_pos, b2_speed, b2_pos, (...),  bs1_pos, bs2_pos, bs3_pos, bs4_pos]\n",
    "    new_state = [0] * (1 + 2 * self.bus_num) + self.bus_stop_positions[0]\n",
    "    personsWaitingTimeList = []\n",
    "    traci.simulationStep()\n",
    "\n",
    "    vehicles = traci.vehicle.getIDList()\n",
    "    persons = traci.person.getIDList()\n",
    "\n",
    "    # finds the average waiting time\n",
    "    for i in range(0, len(persons)):\n",
    "      personWaitingTime = traci.person.getWaitingTime(persons[i])\n",
    "      personsWaitingTimeList.append(personWaitingTime)\n",
    "\n",
    "    persons_waiting_num = len(personsWaitingTimeList)\n",
    "    new_state[0] = round(sum(personsWaitingTimeList) /\n",
    "                         persons_waiting_num, 3) if persons_waiting_num > 0 else 0.0\n",
    "\n",
    "    # finds bus speed and position\n",
    "    for j in range(0, len(vehicles)):\n",
    "      vehicleId = vehicles[j]\n",
    "      if traci.vehicle.getRouteID(vehicleId) != \"r_0\":\n",
    "        continue\n",
    "\n",
    "      vehicleSpeed = traci.vehicle.getSpeed(vehicleId)*3.6  # m/s to km/h\n",
    "      vehiclePosition = traci.vehicle.getDistance(vehicleId) % (self.route_lengths[0]\n",
    "                                                                if (traci.vehicle.getRouteID(vehicleId) == \"r_0\") else self.route_lengths[1])\n",
    "      new_state[1 + 2*j] = round(vehicleSpeed, 2)\n",
    "      new_state[2 + 2*j] = round(vehiclePosition, 2)\n",
    "    return new_state\n",
    "\n",
    "  def _find_nearest(self, array, value):  # BUG FIND NEAREST BUS STOP\n",
    "    idx = np.searchsorted(array, value, side=\"left\")\n",
    "    if (idx == len(array) and math.fabs(value - (array[0] + array[idx-1])) < math.fabs(value - array[idx-1])):\n",
    "      return array[0]\n",
    "    elif idx > 0 and math.fabs(value - array[idx-1]) < math.fabs(value - array[idx]):\n",
    "      return array[idx-1]\n",
    "    else:\n",
    "      return array[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register of Custom Enviroment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/gym/envs/registration.py:498: UserWarning: \u001b[33mWARN: Overriding environment SumoEnv-v0 already in registry.\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n"
     ]
    }
   ],
   "source": [
    "# Register the environment\n",
    "import gym.envs.registration\n",
    "\n",
    "gym.envs.register(\n",
    "    id='SumoEnv-v0',\n",
    "    entry_point='Simulation/Python:SumoEnv',\n",
    "    max_episode_steps=500, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL (DQN) Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib.ExitStack at 0x105698ed0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from collections import namedtuple\n",
    "import random\n",
    "\n",
    "# Use Cuda (GPU) if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transition\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "# Replay Buffer\n",
    "class ReplayBuffer: \n",
    "\n",
    "    # Initialize the buffer\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    # Save a transition to the buffer\n",
    "    def push(self, *args):\n",
    "        ''' Save a transition to the buffer '''\n",
    "        self.buffer.append(Transition(*args))\n",
    "\n",
    "    # Sample a batch of transitions\n",
    "    def sample(self, batch_size):\n",
    "        ''' Sample a batch of transitions '''\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "    \n",
    "    # Returns the length of the buffer\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Q Network\n",
    "class DQN (nn.Module): \n",
    "    def __init__(self, n_obersavations, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_obersavations, 128) # Input Layer\n",
    "        self.layer2 = nn.Linear(128, 128) # Hidden Layer\n",
    "        self.layer3 = nn.Linear(128, n_actions) # Output Layer\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.layer1(x.float()))\n",
    "        x = torch.relu(self.layer2(x.float()))\n",
    "        x = self.layer3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "1. BATCH_SIZE is the number of transitions sampled from the replay buffer\n",
    "2. GAMMA is the discount factor as mentioned in the previous section\n",
    "3. EPS_START is the starting value of epsilon\n",
    "4. EPS_END is the final value of epsilon\n",
    "5. EPS_DECAY controls the rate of exponential decay of epsilon, higher means a slower decay\n",
    "6. TAU is the update rate of the target network\n",
    "7. LR is the learning rate of the ``AdamW`` optimizer\n",
    "8. The size of memory for the Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "# The discount factor, batch size, learning rate, target update frequency, and memory size\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 1000\n",
    "TAU = 0.005\n",
    "LR = 1e-4\n",
    "MEMORY_SIZE = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize environment and DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #0.00 (0ms ?*RT. ?UPS, TraCI: 6394ms, vehicles TOT 0 ACT 0 BUF 0)                   \n"
     ]
    }
   ],
   "source": [
    "traci.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Retrying in 1 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Environment variable SUMO_HOME is not set properly, disabling XML validation. Set 'auto' or 'always' for web lookups.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DQN(\n",
       "  (layer1): Linear(in_features=15, out_features=128, bias=True)\n",
       "  (layer2): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (layer3): Linear(in_features=128, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the environment\n",
    "env = SumoEnv()\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "# Get number of actions and observations from gym action space\n",
    "n_actions = env.action_space.shape[0]\n",
    "\n",
    "# This is most likely wrong\n",
    "n_observations = env.observation_space.shape[0]\n",
    "\n",
    "# Initialize the DQN and target network\n",
    "policy_net = DQN(n_observations, n_actions)\n",
    "target_net = DQN(n_observations, n_actions)\n",
    "\n",
    "# Set the target network to have the same weights as the policy network\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "# Set the target network to evaluation mode\n",
    "target_net.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize optimizer and replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the optimizer\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=LR)\n",
    "\n",
    "# memory is initialized \n",
    "memory = ReplayBuffer(MEMORY_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def select_action(state): \n",
    "#     global steps_done\n",
    "    \n",
    "#     ''' Select an action using an epsilon greedy policy '''\n",
    "\n",
    "#     sample = random.random()\n",
    "#     eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY)\n",
    "#     steps_done += 1\n",
    "\n",
    "#     if sample > eps_threshold:\n",
    "#         with torch.no_grad(): \n",
    "\n",
    "#             # Get predicted Q-values for all possible actions for all agents\n",
    "#             q_values = policy_net(state)\n",
    "\n",
    "#             # Reshape to get individual actions for each agent\n",
    "#             # Assuming 5 agents with 3 possible actions each\n",
    "#             q_values = q_values.view(-1, 5, 3)\n",
    "\n",
    "#             # Select the action with the highest Q-value for each agent\n",
    "#             actions = torch.argmax(q_values, dim=2)\n",
    "\n",
    "#             # Return the selected actions as a single tensor\n",
    "#             return actions.view(1, -1)\n",
    "        \n",
    "#     else:\n",
    "#         # Sample actions for all agents during exploration\n",
    "#         actions = torch.tensor([[(env.action_space.sample())\n",
    "#         for _ in range(5)]], device=device, dtype=torch.long)\n",
    "#     print(\"Exploration: Sampled Actions:\", actions)  # Remove .item() here\n",
    "#     return actions\n",
    "\n",
    "\n",
    "def select_action(state):\n",
    "  global steps_done\n",
    "\n",
    "  sample = random.random()\n",
    "  eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "      math.exp(-1.0 * steps_done / EPS_DECAY)\n",
    "  steps_done += 1\n",
    "\n",
    "  if sample > eps_threshold:\n",
    "    with torch.no_grad():\n",
    "      # Get predicted Q-values for all possible actions for all agents\n",
    "      q_values = policy_net(state)\n",
    "\n",
    "      # Reshape to get individual actions for each agent (bus)\n",
    "      q_values = q_values.view(-1, 5, 3)  # Assuming 5 buses and 3 actions each\n",
    "\n",
    "      # Select the action with the highest Q-value for each bus\n",
    "      actions = torch.argmax(q_values, dim=2)\n",
    "\n",
    "      # Return the list of selected actions\n",
    "      # Convert tensor to list and remove outer list\n",
    "      return actions.tolist()[0]\n",
    "\n",
    "  else:\n",
    "    # Sample actions for all buses during exploration\n",
    "    actions = [env.action_space.sample() for _ in range(5)]\n",
    "    print(\"Exploration: Sampled Actions:\", actions)\n",
    "    return actions\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # if sample > eps_threshold:\n",
    "    #     with torch.no_grad(): \n",
    "    #         return policy_net(state).max(1).indices.view(1, 1)\n",
    "    # else:\n",
    "    #     print(\"env.action_space.sample()\", env.action_space.sample())\n",
    "    #     return torch.tensor([[int(env.action_space.sample())]], device=device, dtype=torch.long)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_durations = []\n",
    "def plot_durations(show_result=False):\n",
    "    plt.figure(1)\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    if show_result:\n",
    "        plt.title('Result')\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        if not show_result:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that performs a single step of the optimization\n",
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor((tuple(\n",
    "        map(lambda s: s is not None, batch.next_state))), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat(\n",
    "        [s for s in batch.next_state if s is not None])\n",
    "\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_net(\n",
    "            non_final_next_states).max(1).values\n",
    "\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values,\n",
    "                     expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # In-place graident clipping\n",
    "    torch.nn.utils.clip_grad_norm_(policy_net.parameters(), 100)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traning Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #0.00 (0ms ?*RT. ?UPS, TraCI: 656ms, vehicles TOT 0 ACT 0 BUF 0)                    \n",
      " Retrying in 1 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Environment variable SUMO_HOME is not set properly, disabling XML validation. Set 'auto' or 'always' for web lookups.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wait Time:  0\n",
      "Action Selection [2 1 2 2 1]\n",
      "Exploration: Sampled Actions: [array([2, 2, 0, 1, 2], dtype=int32), array([2, 1, 1, 2, 0], dtype=int32), array([0, 1, 1, 2, 2], dtype=int32), array([1, 0, 0, 1, 2], dtype=int32), array([1, 2, 2, 1, 0], dtype=int32)]\n",
      "action:  [array([2, 2, 0, 1, 2], dtype=int32), array([2, 1, 1, 2, 0], dtype=int32), array([0, 1, 1, 2, 2], dtype=int32), array([1, 0, 0, 1, 2], dtype=int32), array([1, 2, 2, 1, 0], dtype=int32)]\n",
      "env.action_space.sample() [2 1 0 0 1]\n",
      "Step: 0, wait_time: tensor([0]), Action: [array([2, 2, 0, 1, 2], dtype=int32), array([2, 1, 1, 2, 0], dtype=int32), array([0, 1, 1, 2, 2], dtype=int32), array([1, 0, 0, 1, 2], dtype=int32), array([1, 2, 2, 1, 0], dtype=int32)]\n",
      "\n",
      "Action from env:  [array([2, 2, 0, 1, 2], dtype=int32), array([2, 1, 1, 2, 0], dtype=int32), array([0, 1, 1, 2, 2], dtype=int32), array([1, 0, 0, 1, 2], dtype=int32), array([1, 2, 2, 1, 0], dtype=int32)]\n",
      "An error occurred. Closing simulation.\n",
      "Error:  The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "Step #1.00 (1ms ~= 1000.00*RT, ~1000.00UPS, TraCI: 29ms, vehicles TOT 1 ACT 1 BUF 1)      \n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 24\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menv.action_space.sample()\u001b[39m\u001b[38;5;124m\"\u001b[39m, env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39msample())\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStep: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, wait_time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwait_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Action: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maction\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 24\u001b[0m wait_time, reward, done, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     25\u001b[0m reward \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([reward], device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "from itertools import count\n",
    "\n",
    "num_episodes = 500 \n",
    "episode_durations = []\n",
    "\n",
    "f = open(\"results.csv\", \"w\")\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    \n",
    "    wait_time, info = env.reset()\n",
    "\n",
    "    print(\"Wait Time: \", wait_time)\n",
    "\n",
    "    wait_time = torch.tensor([wait_time], device=device)\n",
    "\n",
    "    for t in count():\n",
    "        print(\"Action Selection\", env.action_space.sample())\n",
    "        action = select_action(wait_time)\n",
    "        print(\"action: \", action)\n",
    "        print(\"env.action_space.sample()\", env.action_space.sample())\n",
    "\n",
    "        print(f\"Step: {t}, wait_time: {wait_time}, Action: {action}\\n\")\n",
    "\n",
    "        wait_time, reward, done, _ = env.step(action)\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
